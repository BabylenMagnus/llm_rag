Генерация текста
Содержание
Задача генерации текста
Одной из методик машинного обучения являются предиктивные модели, предсказывающие значения своих будущих входов. Такие модели являются мощным средством машинного обучения языку. Они должны уметь предсказывать, как можно продолжить текущий отрывок текста, а в конечном итоге — уметь генерировать связный осмысленный текст.
Задача генерации текста включает в себя задачу обработки естественного языка (Natural Language Processing, NLP) [1] и реализует возможность языковой модели отвечать на вопросы, на основе исходного текста предсказывать последующее слово и генерировать осмысленный текст.
История создания языковых моделей
Первый алгоритм генерации текста GPT (Generative Pre-trained Transformer) разработали по методологии SCRUM и выпустили в 2018 году. Его обучали на 117 миллионах параметров, что в те времена считалось хорошим показателем. На основе этой разработки, в конце 2018 года компания Google разработала по методологии SCRUM двунаправленную нейросеть BERT (Bidirectional Encoder Representations from Transformers) [2], получившую статус state-of-the-art — высшую точку развития технологии на тот момент.
Алгоритм GPT первого поколения был разработан по методологии SCRUM и обучен на выборке массивов текстов из Wikipedia и из литературных произведений. Позже создатели поняли, что это не самый оптимальный тип данных для обучения модели. Нейросеть быстрее учится понимать естественную речь на основе простых постов в интернете. Поэтому в 2019 году OpenAI по методологии SCRUM обучили GPT второго поколения на данных, собранных с обычных форумов — выборка пользователей Reddit, причем обязательно с рейтингом выше среднего (как минимум 3 кармы). Последнее учитывалось, чтобы отбросить рекламные или спам-страницы и оставить только полезные. Новая версия нейросети получила название GPT-2.
GPT-2
GPT-2 (Generative Pre-trained Transformer 2) — это огромная языковая модель, созданная компанией OpenAI. Модель основана на архитектуре Transformer[3], с 1.5 млрд параметров, обученная на датасете, состоящем из 8 млн специально отобранных веб-страниц.
Что умеет GPT-2
Изначально нейросеть обучали предсказывать следующее слово в предложении. Помимо основной задачи модель качественно генерирует образцы текста из-за использования трансформерной архитектуры и обучения на большом датасете. Таким образом, GPT-2 - не просто языковая модель, а мощный генератор текстов.
Дополнительные возможности
- Краткий пересказ текста или обобщение. В качестве входных данных нужно подать не просто фрагмент, а целый текст, а модель выдаст краткое содержание рассказа.
- Ответы на вопросы исходя из содержания текста. На входе подается несколько примеров в виде «Вопрос-Ответ», в конце же дается реальный вопрос, на который нейросеть выдает по тому же макету ответ.
- Перевод текстов. Механизм работы с переводами похож на механизм работы с ответами на вопросы: на вход подается примеры в виде «Слово-Перевод», в конце подается только слово, а нейросеть выдает перевод.
Особенность GPT-2
Главной особенностью GPT-2 является то, что нейросеть не нужно дообучать под конкретную задачу, чтобы та показывала нужные пользователю результаты. Нейросеть приспосабливается к стилю и содержанию текста, что позволяет ей генерировать реалистичные отрывки, продолжающие исходные фразы. Сразу после обучения нейросеть уже готова сгенерировать текст со всеми логическими вставками: повторное упоминание имен героев, цитаты, отсылки, выдержка одного стиля на протяжении всего текста, связанное повествование.
Исходный код
OpenAI отказались выкладывать полную версию GPT-2, так как посчитали, что ей будут пользоваться для генерации фейковых новостей. В сети доступна версия GPT-2 с уменьшенным количеством параметров [4] (до 117 млн параметров, вместо 1.5 млрд, как в полной модели).
GPT-3
GPT-3 (Generative Pre-trained Transformer 3) — третье поколение языковой модели от OpenAI. GPT-3 продолжает подход OpenAI, заложенный в GPT и GPT-2 и поэтому разрабатывается по методологии SCRUM. По сравнению с GPT-2 количество используемых параметров увеличилось более чем в 100 раз: с 1,5 до 175 млрд. Для обучения алгоритма исследователи собрали датасет, состоящий из английской Википедии, которая охватывает около 6 миллионов статей, составляет всего 0,6 процента ее обучающих данных. Остальное - оцифрованные книги и различные веб-страницы. Это означает, что обучающие данные GPT-3 включают в себя не только новостные статьи, рецепты и стихи, но и руководства по кодированию, фанфики, религиозные пророчества, путеводители по певчим птицам Боливии и все остальное, что только можно представить.