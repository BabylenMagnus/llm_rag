PixelRNN и PixelCNN
PixelRNN и PixelCNN — алгоритмы машинного обучения, входящие в семейство авторегрессивных моделей и использующиеся для генерации и дополнения изображений. Алгоритмы были представлены в 2016 году компанией DeepMind[1] и являются предшественниками алгоритма WaveNet[2], который используется в голосовом помощнике Google.
Основным преимуществом PixelRNN и PixelCNN является уменьшение времени обучения, по сравнению с наивными способами попиксельной генерации изображений.
Содержание
- 1 Постановка задачи
- 2 Идея
- 3 Архитектура
- 4 Сравнение подходов
- 5 Примеры реализации
- 6 См. также
- 7 Примечания
- 8 Источники информации
Постановка задачи
Пусть дано черно-белое изображениеразмером . Построчно преобразуем картинку в вектор , соединяя конец текущей строки с началом следующей. При таком представлении изображения можно предположить, что значение любого пикселя может зависеть от значений предыдущих пикселей .
Тогда значение пикселя [3]. Оценка совместного распределения всех пикселей будет записываться в следующем виде: .можно выразить через условную вероятность и цепное правило для вероятностей
Задача алгоритма — восстановить данное распределение. Учитывая тот факт, что любой пиксель принимает значение, необходимо восстановить лишь дискретное распределение.
Идея
Так как утверждается, что значение текущего пикселя зависит от значения предыдущего, то уместно использовать рекуррентные нейронные сети (RNN), а точнее долгую краткосрочную память (LSTM). В ранних работах[4] уже использовался данный подход, и вычисление скрытого состояния происходило следующим образом: , т.е. для того, чтобы вычислить текущее скрытое состояние, нужно было подсчитать все предыдущие, что занимает достаточно много времени.
У алгоритма LSTM существует две модификации: RowLSTM и Diagonal BiLSTM. Основным преимуществом модификаций является возможность проводить вычисления параллельно, что ускоряет общее время обучения модели.
RowLSTM
В данной модификации LSTM скрытое состояние считается по формуле: .
Как видно из формулы и Рисунка 2, значение текущего скрытого состояния не зависит от предыдущего слева, а зависит только от предыдущих сверху, которые считаются параллельно.
Таким образом, главным преимуществом алгоритма перед наивным LSTM является более быстрое обучение модели, однако качество получаемых изображений ухудшается. Это связанно как минимум с тем, что мы используем контекст пикселей с предыдущей строки, но никак не используем контекст соседнего слева пикселя, которые является достаточно важным, т.к. является ближайшим с точки зрения построчной генерации изображения. Значит надо научиться находить скрытое состояние слева, но делать это эффективно.
Diagonal BiLSTM
В данной версии скрытое состояние считается таким же образом, как и в наивном подходе:, но использует следующую хитрость в самом вычислении — построчно сдвинем строки вправо на один пиксель относительно предыдущей, а затем вычислим скрытые состояния в каждом столбце, как показано на Рисунке 3. Как следствие, контекст учитывается более качественно, что повышает качество изображения, однако такая модификация замедляет модель по сравнению с подходом RowLSTM.
PixelCNN
Идея в том, что наиболее важные данные для пикселя содержатся в соседних пикселях (в рамках ядра 9x9), поэтому предлагается просто использовать известные пиксели для вычисления нового, как показано на рисунке 2.
Архитектура
В алгоритмах PixelRNN и PixelCNN используются несколько архитектурных трюков, позволяющих производить вычисления быстро и надежно.
Маскированные сверточные слои
В описаниях алгоритмов фигурируют два типа маскированных сверточных слоя — MaskA, MaskB. Они необходимы для сокрытия от алгоритма лишней информации и учета контекста — чтобы ускорить обработку изображения после каждого подсчета, предлагается вместо удаления значения пикселей применять маску к изображению, что является более быстрой операцией.
Для каждого пикселя в цветном изображении в порядке очереди существуют три контекста: красный канал, зеленый и синий. В данном алгоритме очередь важна, т.е. если сейчас обрабатывается красный канал, то контекст только от предыдущих значений красного канала, если зеленый — то от всех значений на красном канале и предыдущих значениях на зеленом и т.д.
MaskA используется для того, чтобы учитывать контекст предыдущих каналов, но при этом не учитывать контекст от предыдущих значений текущего канала и следующих каналов. MaskB выполняет ту же функцию, что и MaskA, но при этом учитывает контекст от предыдущих значений текущего канала.
Уменьшение размерности
На вход в любой их указанных выше алгоритмов (PixelCNN, RowLSTM, Diagonal BiLSTM) подается большое количество объектов, поэтому внутри каждого из них сначала происходит уменьшение их количества в два раза, а затем обратное увеличение до исходного размера. Структура алгоритма с учетом уменьшения размерности показана на рисунке 4.
Внутреннее устройство LSTM
Внутреннее устройство RowLSTM и Diagonal BiLSTM блоков одинаково, за исключением того, что во втором случае добавляется операция сдвига в начале и возврат к исходной структуре изображения в конце.
Структура LSTM блока:
- MaskB слой input-to-state учитывает контекст из входа.
- Сверточный слой state-to-state учитывает контекст из предыдущих скрытых слоев.
Используя эти два сверточных слоя формально вычисление LSTM блока можно записать следующим образом:
где— функция активации,
— операция свертки,
— поэлементное умножение,
— вектор вентиля забывания, вес запоминания старой информации,
— вектор входного вентиля, вес получения новой информации,
— вектор выходного вентиля, кандидат на выход,
— вектор вентиля данных,
— строка входных данных,
— вектор краткосрочной памяти,
— вектор долгосрочной памяти,
и — ядерные веса компонент input-to-state и state-to-state соответственно.
Архитектура PixelRNN
- MaskA размером .
- Блоки уменьшения размеренности с RowLSTM блоком, в котором имеет размер , — . Для Diagonal BiLSTM имеет размер. , — . Количество блоков варьируется.
- ReLU активация.
- Сверточный слой размером .
- Softmax слой.
Архитектура PixelCNN
- MaskA размером .
- Блоки уменьшения размеренности для PixelCNN.
- ReLU активация.
- Сверточный слой размером .
- Softmax слой.
Сравнение подходов
Если сравнивать GAN с PixelCNN/PixelRNN, то можно отметить более хорошее качество получаемых изображений у генеративно-состязательного метода. Однако у метода GAN время обучения медленнее, чем у PixelCNN и PixelRNN. Для реализации GAN требуется найти равновесие Нэша, но в настоящее время нет алгоритма делающего это. Поэтому обучение GAN более нестабильное, если сравнивать с другими методами[7]. В настоящее время многие мировые компании используют GAN для генерации изображений, например: PGGAN от Nvidia, Exemplar GAN от Facebook и другие.
|Критерий\название
|PixelCNN
|PixelRNN(Row LSTM)
|PixelRNN(Diagonal BiLSTM)
|GAN
|Время обучения
|Быстрый
|Средний
|Медленный
|Медленный
|Качество генерируемых изображений
|Наихудшее
|Средне-низкое
|Средне-высокое
|Высокое
Примеры реализации
См. также
- Рекуррентные нейронные сети
- Долгая краткосрочная память
- Нейронные сети, перцептрон
- Генерация объектов